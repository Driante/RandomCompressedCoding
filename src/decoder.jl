using DrWatson
@quickactivate "Random_coding"
using Flux
using Parameters: @with_kw
using ProgressMeter: Progress, next!
include(srcdir("network.jl"))

##Create dataset of noisy responses

function iidgaussian_dataset(n::Network,η ::Float64; ntrn=50000,ntst = 10000,bsize=50,onehot=true)
    #Generate a dataset of noisy response with iid gaussian noise of variance η, with their associated
    #true stimulus. If onehot is true, the stimulus is given as onehot vector
    #Return a DataLoader object iterating over minibatches of bsize.
    U,V = compute_tuning_curves(n,x_test)
    N,L = size(V)
    itrn,itst = rand(1:length(x_test),ntrn),rand(1:length(x_test),ntst)
    r_trn=  V[:,itrn] .+ sqrt(η)*randn(N,ntrn)
    r_tst = V[:,itst] .+ sqrt(η)*randn(N,ntst)
    x_trn,x_tst = x_test[itrn],x_test[itst];
    if onehot
        x_trn = Flux.onehotbatch(x_trn,x_test);
        x_tst =  Flux.onehotbatch(x_tst,x_test);
    end
    data_trn = Flux.Data.DataLoader((r_trn,x_trn),batchsize = bsize);
    data_tst = Flux.Data.DataLoader((r_tst,x_tst),batchsize = bsize);
    return data_trn,data_tst
end

##Decoders
function ideal_decoder_iidgaussian(n::Network,η::Float64,xM ::AbstractArray)
    ~,V = compute_tuning_curves(n,xM)
    λ = V'/η;
    b = -sum((V').^2,dims=2)/(2*η)
    return λ,b
end

function ideal_loss(n,η,x_test,data)
    #Compute losses (cross entropy and mse) on a dataset
    λ_id,b_id = ideal_decoder_iidgaussian(n,η,x_test);
    ce_l = [];
    mse_l = [];
    for d in data
        h = Flux.softmax(λ_id*d[1] .+ b_id)
        push!(ce_l,Flux.Losses.crossentropy(h,d[2]))
        x_ext = x_test'*h
        push!(mse_l,Flux.Losses.mse(Flux.onecold(d[2],x_test)',x_ext))
    end
    return mean(ce_l),mean(mse_l)
end

##Losses
#Loss functions for the probabilistic decoder (outputing the posterior)

#Cross entropy loss
ce_loss(prob_decoder,r,x) = Flux.Losses.crossentropy(prob_decoder(r),x)


function mse_loss_prob(prob_decoder,r,x)
    #COmpute MSE for a decoder which output the posterior over the responses, computing the average of the psoterior distribtuion
    h = prob_decoder(r)
    x_ext = x_test'*h
    return Flux.mse(x_ext,Flux.onecold(x,x_test)')
end

#MSE for a decoder outputing a scalar estimate of the stimulus. True responses are given under the form of one-hot vectors.
mse_loss(mlp_decoder,r,x) =  Flux.mse(mlp_decoder(r),Flux.onecold(x,x_test)')

## Train decoder

@with_kw mutable struct Args
    η = 1e-2               # learning rate
    λ = 0.0f0              # regularization paramater
    epochs = 20             # number of epochs
    M = 500                # latent dimension
    verbose_freq = 10       # logging for every verbose_freq iterations
end

function train_prob_decoder(data; prob_decoder=nothing,kws...)
    #Train a probabilistic decoder on the data generated by the RFFN
    data_trn,data_tst = data[1],data[2]
    args = Args(; kws...)
    if isnothing(prob_decoder)
        prob_decoder = Chain(Dense(N,args.M),softmax)   #Decoder is a linear network which output a probability distribution over M values
    end
    opt = ADAM(args.η)                       #Optmizer for gradient descent
    ps = Flux.params(prob_decoder);    #Collect parameters
    history = Dict(:ce_trn => Float32[],:ce_tst => Float32[],:mse_tst =>Float32[])     #Dictionary to trace progress of training
    trn_step =0
    for epoch = 1:args.epochs
        @info "Epoch $(epoch)"
        progress = Progress(length(data_trn))
        for d in data_trn
            l, back = Flux.pullback(ps) do
                ce_loss(prob_decoder,d...) + args.λ* sum(x->sum(x.^2), ps)
            end
            grad = back(1f0)
            Flux.Optimise.update!(opt, ps, grad)
            next!(progress; showvalues=[(:loss, l),(:epoch,epoch)])
            if trn_step % args.verbose_freq == 0
                push!(history[:ce_trn],l)
            end
            trn_step +=1
        end
        push!(history[:ce_tst],mean([ce_loss(prob_decoder,dtt...) for dtt in data_tst]))
        push!(history[:mse_tst],mean([mse_loss_prob(prob_decoder,dtt...) for dtt in data_tst]))
    end
    return prob_decoder,history
end


function train_mlp_decoder(data; mlp_decoder = nothing, kws...)
    #Train a generic network decoder on the data generated by the RFFN
    data_trn,data_tst = data[1],data[2]
    args = Args(; kws...)
    if isnothing(mlp_decoder)
        mlp_decoder = Chain(Dense(N,args.M,relu),Dense(args.M,1,identity))     #Decoder is a MLP with hidden layer of size M
    end
    opt = ADAM(args.η)
    ps = Flux.params(mlp_decoder);
    history = Dict(:mse_trn => Float32[],:mse_tst => Float32[])
    trn_step = 0
    for epoch = 1:args.epochs
        @info "Epoch $(epoch)"
        progress = Progress(length(data_trn))
        for d in data_trn
            l, back = Flux.pullback(ps) do
                mse_loss(mlp_decoder,d...) + args.λ* sum(x->sum(x.^2), ps)
            end
            grad = back(1f0)
            Flux.Optimise.update!(opt, ps, grad)
            next!(progress; showvalues=[(:loss, l),(:epoch,epoch)])
            if trn_step % args.verbose_freq == 0
                push!(history[:mse_trn],l)
            end
            trn_step +=1
        end
        push!(history[:mse_tst],mean([mse_loss(mlp_decoder,dtt...) for dtt in data_tst]))
    end
    return mlp_decoder,history
end
